# CSC-305 — Compiler Design

---

## ASSIGNMENT - 3

## Requirements & Dependencies

To build and run the code in this repository you will need the following tools installed on your development machine (instructions assume a Linux environment):

* `flex` — fast lexical analyzer generator
* `bison` — GNU parser generator (yacc-compatible)
* `g++` — GNU C++ compiler
* `libfl-dev` (or equivalent) — development library for linking `flex` generated code
* `make` — build automation utility

Example installation on Ubuntu/WSL:

```bash
sudo apt update
sudo apt install build-essential flex bison libfl-dev make
```

---

## Repo layout

```
.
├── makefile        # build rules
├── README.md
├── README.md (extended — Phase 3: Intermediate Code Generation)
├── run.sh
├── src/
│   ├── lexer.l         # flex lexer source
│   ├── parser.y        # bison parser grammar (phase 1-3: lexical, parsing, and intermediate code actions)
│   ├── lex.yy.c        # generated by flex (target of make)
│   ├── parser.tab.c    # generated by bison (target of make)
│   ├── parser.tab.h    # generated header by bison
│   ├── parser.output   # bison verbose output (conflicts/states)
│   ├── parser          # generated executable (target of make)
├── test/
│   ├── test1.txt
│   ├── test2.txt
│   └── ...             # input test files
└── output/
    ├── output1.txt
    ├── intermediate1.tac
    └── ...             # generated output (symbol & constant tables and intermediate code)
```

---

## Features

* A working **lexical analyzer** (implemented in `lexer.l`) that recognizes C/C++-style tokens: keywords, identifiers, literals, operators and punctuation.
* A complete **parser** (implemented in `parser.y`) that processes tokenized input according to C/C++ grammar rules.
* **Symbol Table Generation**: Tracks all identifiers with their data types, scope (IDENTIFIER, FUNCTION, PARAM, MEMBER), and line numbers.
* **Constant Table Generation**: Records all literal values (integers, doubles, strings, characters) with their types and source locations.
* **Grammar Support**: Handles variable declarations, function definitions, class/struct definitions, expressions, control structures, and more.
* **Error Reporting**: Provides detailed syntax error messages with line numbers and context.
* Simple `Makefile` to generate lexer/parser code and build the executable.

---

### Phase 1 — Lexical Analysis

**Location:** `lexer.l`

**What it does:**

* Tokenizes input source files into tokens such as `INT`, `DOUBLE`, `CHAR`, `IDENTIFIER`, `DECIMAL_LITERAL`, operators (`+`, `-`, `*`, `/`, `==`, `<=`, `&&`, `||`), punctuation (`;`, `,`, `{`, `}`), and various literals.
* Supports C++ keywords (`class`, `public`, `private`, `new`, `delete`), C I/O (`cin`, `cout`), and preprocessor directives.
* Handles invalid identifiers and provides appropriate error tokens.

### Phase 2 — Syntax Analysis & Parsing

**Location:** `parser.y`

**What it does:**

* Parses tokenized input according to comprehensive C/C++ grammar rules.
* Builds **Symbol Table** containing:

  * Variable declarations with data types
  * Function definitions with return types
  * Class/struct definitions
  * Function parameters
  * Member variables
* Builds **Constant Table** recording all literal values found in source.
* Supports complex language constructs: pointers, arrays, nested expressions, control flow, OOP features.

---

### Phase 3 — Intermediate Code Generation (NEW)

**Location:** `parser.y` (semantic actions), output written to `stdout` (appended after tables) and optionally to a separate file `output/<testname>.tac` when redirected.

**What it does:**

This phase extends the parser to emit **intermediate code** — a linearized, low-level representation of the program suitable for later optimization and target-code generation. The intermediate representation implemented here is a classic **Three-Address Code (TAC)** variant.

**Key behaviours added in this phase:**

* **Expression lowering:** Complex expressions are decomposed into sequences of TAC statements that use temporary variables (`t1`, `t2`, ...).
* **Temporary & label generation:** The parser keeps counters for temporaries and labels to produce unique names during semantic actions (e.g. `t1`, `L1`).
* **Control-flow constructs:** `if`, `if-else`, `while`, `for`, `break`, `continue`, and `return` generate conditional and unconditional jumps with labels; backpatching is used to fix up forward jumps.
* **Function calls & parameters:** Arguments are evaluated and passed, and call/return sequences are emitted (e.g. `param x`, `call f, n`, `ret t`).
* **Assignments & memory ops:** Simple assignment (`a = b`) and array/pointer operations are emitted as TAC assignments or address-load/store operations.
* **Integration with Symbol Table:** Intermediate code uses symbol-table entries to access variable names, types, and scopes. Temporaries are recorded as ephemeral symbols if needed.

**Output format (TAC):**

Each intermediate instruction follows one of these common forms:

```
# binary op
t1 = b * c
# unary op
t2 = -t1
# assignment
a = t2
# conditional jump
if a < b goto L1
# unconditional jump
goto L2
# label
L1:
# function call / params
param x
call foo, 2
# return
return t3
```

**Example**

Source

```c
int a = b + c * d;
```

TAC (example)

```
t1 = c * d
t2 = b + t1
a = t2
```

Control flow example (if-else)

Source

```c
if (x < y) x = x + 1; else x = x - 1;
```

TAC (example)

```
if x < y goto L1
goto L2
L1:
t1 = x + 1
x = t1
goto L3
L2:
t2 = x - 1
x = t2
L3:
```

**Backpatching & boolean short-circuiting**

To generate correct jump targets for forward-referenced labels (e.g. the else-clause target before it is known) the parser uses a simple backpatch list mechanism stored in semantic attributes. Boolean operators (`&&`, `||`) are lowered using short-circuit semantics into conditional jumps.

**Files produced / where to look**

* `stdout` — by default the parser prints three sections in order: SYMBOL TABLE, CONSTANT TABLE, and **INTERMEDIATE CODE**. Redirect stdout to capture all three in one file.
* `output/<testname>.tac` — a convenience naming convention we recommend: redirect the parser output for a given test into a `.tac` file. Example:

```bash
./parser < test/test1.txt > output/test1.intermediate.txt
```

**How to build & run (Phase 3 additions)**

Building the project remains the same (no new compiler tools needed):

```bash
make
```

Running the parser now produces intermediate code along with the tables. Example:

```bash
./parser < test/test1.txt > output/test1.full.txt
# inspect the intermediate portion at the end of output/test1.full.txt, or
# keep only TAC by filtering the output if you want
```

If you prefer the intermediate code in a separate file in the `output/` folder, run:

```bash
./parser < test/test1.txt | sed -n '/INTERMEDIATE CODE/,$p' > output/test1.tac
```

**Testing & Validation**

* Add small testcases that exercise expressions, nested expressions, function calls, and each control construct. Example tests are included in `test/` and should be extended to validate TAC correctness.
* Confirm that temporaries are unique per expression and that labels match jump targets.
* Use the symbol and constant tables printed earlier in the output to validate operand names/types used in TAC.

**Return codes**

* `0` on successful parsing and intermediate code emission
* Non-zero on syntax errors, semantic errors (where detected), or file I/O issues

---

## Running the project

### Build

From the project root:
generate lex.yy.c, parser.tab.c and build the executable

```bash
make
```

Makefile targets:

* `parser` — runs `bison -d -v` and `flex`, then compiles with `g++ -lfl` to create `parser` executable.
* `clean` — removes generated files (`lex.yy.c`, `parser.tab.c`, `parser.tab.h`, `parser.output`, `parser`).

```bash
make clean
```

removes all generated files

### Run

usage: ./parser < input_file > output_file

```bash
./parser < test/test1.txt > output/output1.txt
```

### Batch runs

A convenience script `run.sh` runs the lexer on multiple `test/` files and writes output to `output/`:

```bash
chmod +x run.sh
./run.sh
# run.sh runs the lexer on multiple test files and writes them to `output/`
```

### Output format

The parser generates three formatted ASCII tables/sections to stdout:

#### 1. Symbol Table

SYMBOL TABLE

```
---------------------------------------------------------------
NAME                DATATYPE            TYPE                LINE      
---------------------------------------------------------------
a                   INT                 IDENTIFIER          4         
b                   INT                 IDENTIFIER          4   
```

#### 2. Constant Table

CONSTANT TABLE

```
---------------------------------------------------------------
VALUE                    TYPE                LINE      
---------------------------------------------------------------
5                        DECIMAL_LITERAL     4         
7                        DECIMAL_LITERAL     4         
"Greater than"           STRING_LITERAL      7
```

#### 3. Intermediate Code (Three-Address Code)

```
# a sequence of TAC instructions, labels and jumps as shown above
```

### Return codes

* `0` on successful parsing with table generation and TAC emission
* Non-zero on syntax errors or file I/O issues

---

### Contributors

* Kartik Sarda - 23114047
* Nitin Agiwal - 23114074
* Adesh Kaduba Palkar - 23114004
* Utkarsh Kumar - 23114101

