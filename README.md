# CSC-305-Compiler-Design
# Lexer

A small lexer (tokenizer) implemented with **flex** and C++ that reads a source file and produces a nicely formatted token table plus a separate error file for invalid tokens/identifiers. This repo contains the lexer rules (lex.l), a makefile, a run.sh bash script, test inputs, and an output/ directory that stores results.

---

## Features

* Recognizes common C/C++-like tokens (operators, punctuation, keywords, literals, identifiers).
* Classifies invalid tokens and invalid identifiers into a separate error file.
* Produces a human-readable ASCII table with `Line No | Lexeme | Token`.
* Simple build with `make` and a batch-run script `run.sh`.

---

## Repo layout

```
.
├── makefile          # build rules
├── run.sh            # convenience script to run lexer on all tests
├── README.md
├── src/
│   ├── lex.l         # flex lexer source
│   ├── lex.yy.c      # generated by flex (target of make)
│   └── lexer         # generated executable (target of make)
├── test/
│   ├── test1.txt
│   ├── test2.txt
│   └── ...           # input test files
└── output/
    ├── output1.txt   # generated output tables
    └── error_output1.txt
```

---

## Requirements / Dependencies

* `flex`
* `g++` (C++ compiler)
* `libfl` development library (on Ubuntu: `libfl-dev`)

Example install on Ubuntu:

```bash
sudo apt update
sudo apt install build-essential flex libfl-dev
```

---

## Build

From the project root:

```bash
# generate src/lex.yy.c and build the executable
make
```

Makefile targets:

* `lexer` — runs `flex` to generate `src/lex.yy.c` and compiles it with `g++ -lfl` to `src/lexer`.
* `clean` — removes generated files:

```bash
make clean
# removes src/lex.yy.c and src/lexer
```

---

## Usage

Single run:

```bash
# usage: ./src/lexer <input_file> <output_file>
./src/lexer test/test1.txt output/output1.txt
```

This will also write errors to `error_<output_file>` (for example `error_output1.txt`).

**Batch runs (pre-written):**

```bash
chmod +x run.sh
./run.sh
# run.sh runs the lexer on multiple test files and writes them to `output/`
```

Return codes:

* `0` on success
* `1` on usage or file-open errors

---

## Output format

Each run writes two files:

1. `<output_file>` — formatted ASCII table of recognized tokens:

```
+---------+---------------------+--------------------------+
| Line No | Lexeme              | Token                    |
+---------+---------------------+--------------------------+
| 1       | #include <stdio.h>  | INCLUDE                  |
+---------+---------------------+--------------------------+
| 2       | int                 | INT                      |
+---------+---------------------+--------------------------+
...
```

2. `error_<output_file>` — same header but lists tokens classified as `INVALID_IDENTIFIER` or `INVALID_TOKEN`.

---

## Example commands summary

```bash
# build
make

# run single test
./src/lexer test/test1.txt output/output1.txt

# run all tests listed in run.sh
chmod +x run.sh
./run.sh

make clean
```

---

## Conclusion

This lexer is a lightweight, easy-to-extend tokenizer for C/C++-like source files — ideal as a learning tool or the first stage of a compiler toolchain. It outputs clear, human-readable token and error tables, and the code is intentionally simple so you can quickly add new tokens or improve handling.

---
