# CSC-305 — Compiler Design

---

## ASSIGNMENT - 3

## Requirements & Dependencies

To build and run the code in this repository you will need the following tools installed on your development machine (instructions assume a Linux environment):

* `flex` — fast lexical analyzer generator
* `bison` — GNU parser generator (yacc-compatible)
* `g++` — GNU C++ compiler
* `libfl-dev` (or equivalent) — development library for linking `flex` generated code
* `make` — build automation utility

Example installation on Ubuntu/WSL:

```bash
sudo apt update
sudo apt install build-essential flex bison libfl-dev make
```

---

## Repo layout

```
.
├── makefile        # build rules
├── README.md
├── run.sh
├── src/
│   ├── lexer.l         # flex lexer source
│   ├── parser.y        # bison parser grammar (helper functions to generate IR)
│   ├── lex.yy.c        # generated by flex (target of make)
│   ├── parser.tab.c    # generated by bison (target of make)
│   ├── parser.tab.h    # generated header by bison
│   ├── parser.output   # bison verbose output (conflicts/states)
│   ├── parser          # generated executable (target of make)
├── test/
│   ├── test1.txt
│   ├── test2.txt
│   └── ...             # input test files
└── output/
    ├── output1.txt
    └── ...             # generated output (intermediate code)
```

---

## Features

* A working **lexical analyzer** (implemented in `lexer.l`) that recognizes C/C++-style tokens: keywords, identifiers, literals, operators and punctuation.
* A complete **parser** (implemented in `parser.y`) that processes tokenized input according to C/C++ grammar rules.
* **Symbol Table Generation**: Tracks all identifiers with their data types, scope (IDENTIFIER, FUNCTION), types and kind.
* **Grammar Support**: Handles variable declarations, function definitions, class/struct definitions, expressions, control structures, and more.
* **Semantic Error Reporting**: Checks for various semantic errors ad provides detailed error messages with line numbers and context.
* Simple `Makefile` to generate lexer/parser code and build the executable.

---

### Phase 1 — Lexical Analysis

**Location:** `lexer.l`

**What it does:**

* Tokenizes input source files into tokens such as `INT`, `DOUBLE`, `CHAR`, `IDENTIFIER`, `DECIMAL_LITERAL`, operators (`+`, `-`, `*`, `/`, `==`, `<=`, `&&`, `||`), punctuation (`;`, `,`, `{`, `}`), and various literals.
* Supports C++ keywords (`class`, `public`, `private`, `new`, `delete`), C I/O (`cin`, `cout`), and preprocessor directives.
* Handles invalid identifiers and provides appropriate error tokens.

### Phase 2 — Syntax Analysis & Parsing

**Location:** `parser.y`

**What it does:**

* Parses tokenized input according to comprehensive C/C++ grammar rules.
* Builds **Symbol Table** containing:

  * Variable declarations with data types
  * Function definitions with return types
  * Class/struct definitions
  * Function parameters
  * Member variables
* Builds **Constant Table** recording all literal values found in source.

---

### Phase 3 — Intermediate Code Generation

**Location:** `parser.y` (semantic actions), output written to `output.txt`

**What it does:**

This phase extends the parser to emit **intermediate code** — a linearized, low-level representation of the program suitable for later optimization and target-code generation. The intermediate representation implemented here is a classic **Three-Address Code (TAC)** variant.

**Key behaviours added in this phase:**

* **Expression lowering:** Complex expressions are decomposed into sequences of TAC statements that use temporary variables (`t1`, `t2`, ...).
* **Temporary & label generation:** The parser keeps counters for temporaries and labels to produce unique names during semantic actions (e.g. `t1`, `L1`).
* **Control-flow constructs:** `if`, `if-else`, `while`, `for`, `break`, `continue`, and `return` generate conditional and unconditional jumps with labels; backpatching is used to fix up forward jumps.
* **Function calls & parameters:** Arguments are evaluated and passed, and call/return sequences are emitted (e.g. `param x`, `call f, n`, `return t`).
* **Assignments & memory ops:** Simple assignment (`a = b`) and array/pointer operations are emitted as TAC assignments or address-load/store operations.
* **Integration with Symbol Table:** Intermediate code uses symbol-table entries to access variable names, types, and scopes. Temporaries are recorded as ephemeral symbols if needed.

**Backpatching**

To generate correct jump targets for forward-referenced labels (e.g. the else-clause target before it is known) the parser uses a simple backpatch list mechanism stored in semantic attributes.

**Modification**

In our compiler we use that case statements use curly braces `{}` instead of a colon `:` to enclose their block.

---

## Running the project

### Build

From the project root:
generate lex.yy.c, parser.tab.c and build the executable

```bash
make
```

### Run

usage: ./parser < input_file > output_file

```bash
./parser < test/test1.txt > output/output1.txt
```

### Batch runs

A convenience script `run.sh` runs the lexer on multiple `test/` files and writes output to `output/`:

```bash
chmod +x run.sh
./run.sh
# run.sh runs the lexer on multiple test files and writes them to `output/`
```

### Output format

```bash

main:
    param "%d %d"
    main.t1 = &main.a
    param main.t1
    main.t2 = &main.b
    param main.t2
    main.t3 = call scanf, 3
    main.t4 = main.a + main.b
    main.sum = main.t4
    param "Sum: %d\n"
    param main.sum
    main.t10 = call printf, 2
    return 0
```
### Return codes

* `0` on successful parsing with table generation and TAC emission
* Non-zero on syntax errors or file I/O issues

---

### Contributors

* Adesh Kaduba Palkar - 23114004
* Kartik Sarda - 23114047
* Nitin Agiwal - 23114074
* Utkarsh Kumar - 23114101

